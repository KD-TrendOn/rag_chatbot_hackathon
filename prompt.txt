├── Dockerfile
├── alembic/
│   ├── env.py
│   └── versions/
│       ├── 18142ce122de_add_columns.py
│       ├── 2229705dc9fd_add_foreign_keys_to_chats_and_messages.py
│       └── b4dc58d4648e_initial_migration.py
├── app/
│   ├── __init__.py
│   ├── api/
│   │   ├── __init__.py
│   │   └── endpoints.py
│   ├── chunks_creating.py
│   ├── config.py
│   ├── database/
│   │   ├── __init__.py
│   │   └── vector_store.py
│   ├── dependencies.py
│   ├── graphs/
│   │   ├── __init__.py
│   │   └── main_graph.py
│   ├── logger.py
│   ├── main.py
│   ├── photo_indexes.py
│   ├── preprocessing/
│   │   ├── __init__.py
│   │   └── context_annotator.py
│   ├── schemas/
│   │   ├── __init__.py
│   │   ├── chat.py
│   │   ├── message.py
│   │   ├── state.py
│   │   └── user.py
│   ├── services/
│   │   ├── __init__.py
│   │   ├── llm.py
│   │   └── model_loader.py
│   └── state.py
├── core/
│   ├── __init__.py
│   ├── config.py
│   └── models/
│       ├── __init__.py
│       ├── base.py
│       ├── chat.py
│       ├── crud.py
│       ├── db_helper.py
│       ├── message.py
│       └── user.py
├── docker-compose.yml
├── entrypoint.sh
├── images/
├── logs/
├── requirements.txt
├── uploads/
└── wait-for-it.sh

Dockerfile

# Используем официальный образ Python с поддержкой CUDA
FROM nvidia/cuda:11.7.0-cudnn8-runtime-ubuntu22.04

# Устанавливаем зависимости системы
RUN apt-get update && \
    apt-get install -y --no-install-recommends \
    python3.11 \
    python3.11-venv \
    python3.11-dev \
    build-essential \
    libpq-dev \
    libffi-dev \
    libssl-dev \
    libxml2-dev \
    libxslt1-dev \
    zlib1g-dev \
    poppler-utils \
    tesseract-ocr \
    tesseract-ocr-rus \
    netcat \
    && rm -rf /var/lib/apt/lists/*

# Обновляем альтернативы для Python
RUN update-alternatives --install /usr/bin/python python /usr/bin/python3.11 1
RUN update-alternatives --install /usr/bin/pip pip /usr/bin/pip3 1

# Создаем рабочую директорию
WORKDIR /app

# Копируем файлы зависимостей
COPY requirements.txt .

# Устанавливаем pip
RUN python -m pip install --upgrade pip

# Устанавливаем зависимости Python
RUN pip install --no-cache-dir -r requirements.txt

# Копируем остальные файлы проекта
COPY . .

# Копируем скрипт ожидания
COPY wait-for-it.sh /app/wait-for-it.sh
RUN chmod +x /app/wait-for-it.sh

# Копируем скрипт entrypoint
COPY entrypoint.sh /app/entrypoint.sh
RUN chmod +x /app/entrypoint.sh

# Указываем, что контейнер использует GPU
ENV NVIDIA_VISIBLE_DEVICES=all
ENV NVIDIA_DRIVER_CAPABILITIES=compute,utility

# Определяем команду по умолчанию
CMD ["./entrypoint.sh"]


alembic/env.py


from logging.config import fileConfig

from sqlalchemy import engine_from_config
from sqlalchemy import pool

from alembic import context

from core.config import settings
from core.models.base import Base
from core.models.chat import Chat  # Предполагается, что Base включает все модели
from core.models.user import User
from core.models.message import Message

# this is the Alembic Config object, which provides
# access to the values within the .ini file in use.
config = context.config

# Interpret the config file for Python logging.
# This line sets up loggers basically.
if config.config_file_name is not None:
    fileConfig(config.config_file_name)

# add your model's MetaData object here
# for 'autogenerate' support
target_metadata = Base.metadata

# other values from the config, defined by the needs of env.py,
# can be acquired:
# my_important_option = config.get_main_option("my_important_option")
# ... etc.

def get_url():
    # Создаем синхронный URL для PostgreSQL
    return f"postgresql://{settings.POSTGRES_USER}:{settings.POSTGRES_PASSWORD}@{settings.POSTGRES_HOST}:{settings.POSTGRES_PORT}/{settings.POSTGRES_DB}"

def run_migrations_offline() -> None:
    """Run migrations in 'offline' mode.

    This configures the context with just a URL
    and not an Engine, though an Engine is acceptable
    here as well.  By skipping the Engine creation
    we don't even need a DBAPI to be available.

    Calls to context.execute() here emit the given string to the
    script output.

    """
    url = get_url()
    context.configure(
        url=url,
        target_metadata=target_metadata,
        literal_binds=True,
        dialect_opts={"paramstyle": "named"},
    )

    with context.begin_transaction():
        context.run_migrations()


def run_migrations_online() -> None:
    """Run migrations in 'online' mode.

    In this scenario we need to create an Engine
    and associate a connection with the context.

    """
    configuration = config.get_section(config.config_ini_section)
    configuration["sqlalchemy.url"] = get_url()
    connectable = engine_from_config(
        configuration,
        prefix="sqlalchemy.",
        poolclass=pool.NullPool,
    )

    with connectable.connect() as connection:
        context.configure(
            connection=connection, target_metadata=target_metadata
        )

        with context.begin_transaction():
            context.run_migrations()


if context.is_offline_mode():
    run_migrations_offline()
else:
    run_migrations_online()


alembic/versions/18142ce122de_add_columns.py

"""add columns

Revision ID: 18142ce122de
Revises: 2229705dc9fd
Create Date: 2024-10-27 04:10:34.549605

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa
from sqlalchemy.dialects import postgresql

# revision identifiers, used by Alembic.
revision: str = '18142ce122de'
down_revision: Union[str, None] = '2229705dc9fd'
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.drop_index('ix_cmetadata_gin', table_name='langchain_pg_embedding', postgresql_using='gin')
    op.drop_index('ix_langchain_pg_embedding_id', table_name='langchain_pg_embedding')
    op.drop_table('langchain_pg_embedding')
    op.drop_table('langchain_pg_collection')
    op.add_column('chats', sa.Column('tsv_content', postgresql.TSVECTOR(), nullable=True))
    op.add_column('messages', sa.Column('tsv_content', postgresql.TSVECTOR(), nullable=True))
    op.add_column('users', sa.Column('tsv_content', postgresql.TSVECTOR(), nullable=True))
    # ### end Alembic commands ###


def downgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.drop_column('users', 'tsv_content')
    op.drop_column('messages', 'tsv_content')
    op.drop_column('chats', 'tsv_content')
    op.create_table('langchain_pg_collection',
    sa.Column('uuid', sa.UUID(), autoincrement=False, nullable=False),
    sa.Column('name', sa.VARCHAR(), autoincrement=False, nullable=False),
    sa.Column('cmetadata', postgresql.JSON(astext_type=sa.Text()), autoincrement=False, nullable=True),
    sa.PrimaryKeyConstraint('uuid', name='langchain_pg_collection_pkey'),
    sa.UniqueConstraint('name', name='langchain_pg_collection_name_key'),
    postgresql_ignore_search_path=False
    )
    op.create_table('langchain_pg_embedding',
    sa.Column('id', sa.VARCHAR(), autoincrement=False, nullable=False),
    sa.Column('collection_id', sa.UUID(), autoincrement=False, nullable=True),
    sa.Column('embedding', sa.NullType(), autoincrement=False, nullable=True),
    sa.Column('document', sa.VARCHAR(), autoincrement=False, nullable=True),
    sa.Column('cmetadata', postgresql.JSONB(astext_type=sa.Text()), autoincrement=False, nullable=True),
    sa.ForeignKeyConstraint(['collection_id'], ['langchain_pg_collection.uuid'], name='langchain_pg_embedding_collection_id_fkey', ondelete='CASCADE'),
    sa.PrimaryKeyConstraint('id', name='langchain_pg_embedding_pkey')
    )
    op.create_index('ix_langchain_pg_embedding_id', 'langchain_pg_embedding', ['id'], unique=True)
    op.create_index('ix_cmetadata_gin', 'langchain_pg_embedding', ['cmetadata'], unique=False, postgresql_using='gin')
    # ### end Alembic commands ###


alembic/versions/2229705dc9fd_add_foreign_keys_to_chats_and_messages.py

"""Add foreign keys to chats and messages

Revision ID: 2229705dc9fd
Revises: b4dc58d4648e
Create Date: 2024-10-26 20:54:06.994503

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa
from sqlalchemy.dialects import postgresql

# revision identifiers, used by Alembic.
revision: str = '2229705dc9fd'
down_revision: Union[str, None] = 'b4dc58d4648e'
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.drop_index('ix_cmetadata_gin', table_name='langchain_pg_embedding', postgresql_using='gin')
    op.drop_index('ix_langchain_pg_embedding_id', table_name='langchain_pg_embedding')
    op.drop_table('langchain_pg_embedding')
    op.drop_table('langchain_pg_collection')
    op.create_foreign_key(None, 'chats', 'users', ['user_id'], ['id'])
    op.create_foreign_key(None, 'messages', 'chats', ['chat_id'], ['id'])
    # ### end Alembic commands ###


def downgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.drop_constraint(None, 'messages', type_='foreignkey')
    op.drop_constraint(None, 'chats', type_='foreignkey')
    op.create_table('langchain_pg_collection',
    sa.Column('uuid', sa.UUID(), autoincrement=False, nullable=False),
    sa.Column('name', sa.VARCHAR(), autoincrement=False, nullable=False),
    sa.Column('cmetadata', postgresql.JSON(astext_type=sa.Text()), autoincrement=False, nullable=True),
    sa.PrimaryKeyConstraint('uuid', name='langchain_pg_collection_pkey'),
    sa.UniqueConstraint('name', name='langchain_pg_collection_name_key'),
    postgresql_ignore_search_path=False
    )
    op.create_table('langchain_pg_embedding',
    sa.Column('id', sa.VARCHAR(), autoincrement=False, nullable=False),
    sa.Column('collection_id', sa.UUID(), autoincrement=False, nullable=True),
    sa.Column('embedding', sa.NullType(), autoincrement=False, nullable=True),
    sa.Column('document', sa.VARCHAR(), autoincrement=False, nullable=True),
    sa.Column('cmetadata', postgresql.JSONB(astext_type=sa.Text()), autoincrement=False, nullable=True),
    sa.ForeignKeyConstraint(['collection_id'], ['langchain_pg_collection.uuid'], name='langchain_pg_embedding_collection_id_fkey', ondelete='CASCADE'),
    sa.PrimaryKeyConstraint('id', name='langchain_pg_embedding_pkey')
    )
    op.create_index('ix_langchain_pg_embedding_id', 'langchain_pg_embedding', ['id'], unique=True)
    op.create_index('ix_cmetadata_gin', 'langchain_pg_embedding', ['cmetadata'], unique=False, postgresql_using='gin')
    # ### end Alembic commands ###


alembic/versions/b4dc58d4648e_initial_migration.py

"""Initial migration

Revision ID: b4dc58d4648e
Revises: 
Create Date: 2024-10-26 18:56:48.800126

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa


# revision identifiers, used by Alembic.
revision: str = 'b4dc58d4648e'
down_revision: Union[str, None] = None
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.create_table('chats',
    sa.Column('id', sa.Integer(), autoincrement=True, nullable=False),
    sa.Column('user_id', sa.Integer(), nullable=False),
    sa.PrimaryKeyConstraint('id')
    )
    op.create_table('messages',
    sa.Column('id', sa.Integer(), autoincrement=True, nullable=False),
    sa.Column('chat_id', sa.Integer(), nullable=False),
    sa.Column('sender', sa.String(), nullable=False),
    sa.Column('content', sa.String(), nullable=False),
    sa.Column('timestamp', sa.DateTime(), nullable=False),
    sa.PrimaryKeyConstraint('id')
    )
    op.create_table('users',
    sa.Column('id', sa.Integer(), autoincrement=True, nullable=False),
    sa.Column('user_id', sa.String(), nullable=False),
    sa.PrimaryKeyConstraint('id')
    )
    op.create_index(op.f('ix_users_user_id'), 'users', ['user_id'], unique=True)
    # ### end Alembic commands ###


def downgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.drop_index(op.f('ix_users_user_id'), table_name='users')
    op.drop_table('users')
    op.drop_table('messages')
    op.drop_table('chats')
    # ### end Alembic commands ###


app/__init__.py



app/api/__init__.py



app/api/endpoints.py

# app/api/endpoints.py
import base64
import os
import uuid
from io import BytesIO
from typing import List

import aiofiles
from fastapi import APIRouter, Depends, HTTPException
from pytesseract import pytesseract
from sqlalchemy.ext.asyncio import AsyncSession
from yandex.cloud.ai.vision.v2.image_pb2 import Image

from core.models import User, Chat
from ..graphs.main_graph import worker
from ..schemas.chat import ChatWithLastMessageResponse
from ..schemas.message import GetMessageHistoryResponse, PhotoInfo, SendMessageResponse, SendMessageRequest
from ..schemas.state import State
from ..schemas.user import UserResponse
from core.models.crud import (
    create_user,
    create_chat,
    get_user_by_user_id,
    get_last_five_chats_with_last_message,
    get_all_chat_history_by_chat_id, create_message, get_last_n_messages
)
from core.models.db_helper import db_helper
from ..dependencies import get_model_loader  # Импортируем зависимость

router = APIRouter()

@router.get("/chat_history/{chat_id}", response_model=List[GetMessageHistoryResponse])
async def get_chat_history(chat_id: str, session: AsyncSession = Depends(db_helper.session_dependency)):
    messages = await get_all_chat_history_by_chat_id(session, int(chat_id))
    message_responses = []
    for i in messages:
        message_responses.append(
            GetMessageHistoryResponse(
                sender=i.sender,
                content=i.content,
                timestamp=str(i.timestamp)
            )
        )
    return message_responses

@router.post("/chat-create/{user_id}")
async def create_chat_post(user_id: str, session: AsyncSession = Depends(db_helper.session_dependency)):
    chat = await create_chat(session, User(id=int(user_id)))
    return {"chat_id" : str(chat.id)}

@router.post("/user", response_model=UserResponse)
async def new_user(session: AsyncSession = Depends(db_helper.session_dependency)):
    """
    Создает нового пользователя с уникальным user_id.
    """
    try:
        # Генерация нового UUID
        new_uuid = str(uuid.uuid4())
        new_user = await create_user(session=session, user_id=new_uuid)
        return UserResponse(user_id=str(new_user.id))
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/last/{user_id}", response_model=List[ChatWithLastMessageResponse])
async def get_last_five_chat_by_id(
        user_id: str,
        session: AsyncSession = Depends(db_helper.session_dependency)
):
    """
    Получает последние пять чатов пользователя идентификатора user_id.
    """
    try:
        # Проверяем существование пользователя
        user = await get_user_by_user_id(session, int(user_id))
        if not user:
            raise HTTPException(status_code=404, detail="User not found")

        # Получаем последние пять чатов с последними сообщениями
        chats_with_messages = await get_last_five_chats_with_last_message(session, int(user_id), limit=5)
        return chats_with_messages
    except HTTPException as he:
        raise he
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@router.post("/send_message", response_model=SendMessageResponse)
async def send_message(
        request: SendMessageRequest,
        session: AsyncSession = Depends(db_helper.session_dependency),
        model_loader: ModelLoader = Depends(get_model_loader)  # Подключаем зависимость
):
    """
    Отправляет сообщение пользователем в чат и возвращает ответ от бота.
    Поддерживает отправку изображений в формате Base64 с использованием модели Qwen для извлечения текста.
    """
    user_id = request.user_id
    chat_id = request.chat_id
    message = request.message
    images = request.images

    # Проверка существования пользователя
    user = await get_user_by_user_id(session, int(user_id))
    if not user:
        raise HTTPException(status_code=404, detail="User not found")

    # Проверка существования чата
    chat = await session.get(Chat, int(chat_id))
    if not chat:
        chat = await create_chat(session, user)

    # Обработка отправленных изображений с использованием Qwen
    image_texts = []
    if images:
        for img in images:
            try:
                # Декодирование Base64 в байты
                image_data = base64.b64decode(img.data)

                # Сохранение изображения на сервер
                file_path = os.path.join("uploads", img.filename)
                os.makedirs("uploads", exist_ok=True)
                async with aiofiles.open(file_path, 'wb') as out_file:
                    await out_file.write(image_data)

                # Использование модели Qwen для извлечения текста из изображения
                processor = model_loader.get_processor()
                model = model_loader.get_model()

                # Подготовка сообщений для модели Qwen
                messages_qwen = [
                    {
                        "role": "user",
                        "content": [
                            {
                                "type": "image",
                                "image": file_path,  # Используем локальный путь к сохраненному изображению
                            },
                            {"type": "text", "text": "Describe this image."},
                        ],
                    }
                ]

                # Подготовка к инференсу
                text = processor.apply_chat_template(
                    messages_qwen, tokenize=False, add_generation_prompt=True
                )
                image_inputs, video_inputs = process_vision_info(messages_qwen)
                inputs = processor(
                    text=[text],
                    images=image_inputs,
                    videos=video_inputs,
                    padding=True,
                    return_tensors="pt",
                )

                # Переносим данные на GPU
                inputs = {k: v.to(model.device) for k, v in inputs.items() if isinstance(v, torch.Tensor)}

                # Инференс: Генерация ответа
                with torch.no_grad():
                    generated_ids = model.generate(**inputs, max_new_tokens=128)
                generated_ids_trimmed = [
                    out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs["input_ids"], generated_ids)
                ]
                output_text = processor.batch_decode(
                    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
                )

                image_texts.append(output_text[0])  # Добавляем сгенерированный текст

            except Exception as e:
                raise HTTPException(status_code=500, detail=f"Ошибка обработки изображения {img.filename}: {e}")

    # Комбинирование текста сообщения и текста из изображений
    if image_texts:
        combined_message = f"{message}\n" + "\n".join(image_texts)
    else:
        combined_message = message

    # Сохранение сообщения пользователя в базе данных
    try:
        await create_message(session, chat, "user", combined_message)
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Ошибка сохранения сообщения: {e}")

    # Получение последних 5 сообщений из чата для контекста
    try:
        last_messages_objs = await get_last_n_messages(session, chat, limit=5)
        # Формирование списка кортежей (sender, content)
        last_messages = [(msg.sender, msg.content) for msg in last_messages_objs]
        # Добавление текущего сообщения пользователя для формирования контекста
        last_messages.append(("user", combined_message))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Ошибка получения истории чата: {e}")

    # Инициализация состояния
    initial_state: State = {
        "user_message": combined_message,
        "last_messages": last_messages,
        "is_index": False,
        "answer": "",
        "rel_docs": [],
        "db_query": "",
        "retries": 0,
        "rewrite": False
    }

    # Вызов worker для обработки состояния
    try:
        response_state = await worker.ainvoke(initial_state)
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Ошибка при вызове worker: {e}")

    # Получение ответа бота и его сохранение
    bot_answer = response_state.get("answer", "")
    try:
        await create_message(session, chat, "bot", bot_answer)
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Ошибка сохранения ответа бота: {e}")

    # Обработка rel_docs для отправки фотографий
    photos_info = []
    if response_state.get("rel_docs"):
        for doc in response_state["rel_docs"]:
            for chapter, details in doc.items():
                content = details.get("content", "")
                image_paths = details.get("paths", [])
                for path in image_paths:
                    # Извлечение номера главы и номера рисунка из имени файла
                    filename = os.path.basename(path)
                    parts = filename.split('_')
                    if len(parts) < 2:
                        continue  # Неверный формат имени файла
                    chapter_num = parts[0]
                    image_num_part = parts[1].split('.')[0]  # Пример: 'image1.png' -> 'image1'
                    image_num = ''.join(filter(str.isdigit, image_num_part))  # Извлекаем только цифры

                    # Чтение изображения и кодирование в Base64
                    try:
                        async with aiofiles.open(path, "rb") as image_file:
                            image_bytes = await image_file.read()
                            encoded_string = base64.b64encode(image_bytes).decode('utf-8')
                    except Exception as e:
                        raise HTTPException(status_code=500, detail=f"Ошибка чтения изображения {path}: {e}")

                    photos_info.append(PhotoInfo(
                        chapter=chapter_num,
                        image_number=image_num,
                        base64_data=encoded_string
                    ))

    return SendMessageResponse(
        user_message=combined_message,
        bot_answer=bot_answer,
        chat_id=chat.id,
        photos=photos_info
    )

app/chunks_creating.py

# app/chunks_creating.py
import uuid 
import re
from docx import Document as DocxDocument
from langchain.schema import Document
from app import photo_indexes
import os
from .preprocessing.context_annotator import ContextAnnotator
from langchain.text_splitter import RecursiveCharacterTextSplitter
from rank_bm25 import BM25Okapi
from typing import List
from tqdm import tqdm
# Чтение текста из файла .docx
def load_text_from_docx(file_path):
    doc = DocxDocument(file_path)
    full_text = []
    for para in doc.paragraphs:
        full_text.append(para.text)
    return "\n".join(full_text)

# Путь к твоему файлу .docx
file_path = "documentation_preprocessed.docx"

# Загружаем текст из файла
text_content = load_text_from_docx(file_path)

# titles из твоего словаря заголовков, включая Аннотацию и КОНТАКТНУЮ ИНФОРМАЦИЮ
titles = photo_indexes.titles
titles["Аннотация"] = "Аннотация"
titles["КОНТАКТНАЯ ИНФОРМАЦИЯ"] = "КОНТАКТНАЯ ИНФОРМАЦИЯ"

# Регулярное выражение для поиска заголовков, включая их название (добавлены "Аннотация" и "КОНТАКТНАЯ ИНФОРМАЦИЯ")
heading_pattern = re.compile(r"(\d+(\.\d+)*|Аннотация|КОНТАКТНАЯ ИНФОРМАЦИЯ)")

# Функция для разделения текста на части по заголовкам (включая название)
def split_text_by_headings(text: str, titles: dict):
    documents = []
    matches = list(heading_pattern.finditer(text))
    
    for i in range(len(matches)):
        start = matches[i].start()
        end = matches[i + 1].start() if i + 1 < len(matches) else len(text)
        
        chapter = matches[i].group(1).strip()
        chapter_title = titles.get(chapter, "Неизвестный заголовок")
        
        content = text[start:end].strip()
        image_paths = get_files_by_number(chapter)
        
        doc = Document(
            page_content=content[:1000],
            metadata={
                "id": str(uuid.uuid4()),  # Добавляем уникальный ID
                "content": content,
                "chapter": chapter,
                "title": chapter_title,
                "paths": image_paths
            }
        )
        documents.append(doc)
    
    return documents

# Получение файлов по номеру главы (реализуй эту функцию)
def get_files_by_number(number):
    # Здесь функция get_files_by_number уже должна быть реализована
    # для получения путей к фоткам, которые соответствуют главе
    return [f for f in os.listdir("images") if f.startswith(str(number)+'_')]

# Применяем функцию для разделения текста
async def create_annotated_documents() -> List[Document]:
    # Создаем базовые документы как раньше
    documents = split_text_by_headings(text_content, titles)
    
    # Создаем аннотатор
    annotator = ContextAnnotator()
    
    # Добавляем контекст к каждому документу
    annotated_documents = []
    for doc in tqdm(documents):
        # annotated_text = await annotator.annotate_chunk(
        #     doc.page_content,
        #     doc.metadata
        # )
        annotated_text = "Текст про " + doc.page_content 
        # Store both original content and annotated text
        original_content = doc.page_content
        doc.page_content = annotated_text  # annotated for vectorstore
        annotated_documents.append(doc)
        
        # Create a separate Document for BM25 indexing with original content
        bm25_doc = Document(
            page_content=original_content,
            metadata=doc.metadata  # share metadata
        )
        annotated_documents.append(bm25_doc)
        # annotated_documents = []
    return annotated_documents

def create_bm25_index(documents: List[Document]) -> tuple:
    # Separate original documents for BM25
    original_docs = [doc for doc in documents if doc.page_content == doc.metadata['content']]
    
    # Создаем BM25 индекс
    tokenized_docs = [doc.page_content.split() for doc in original_docs]
    bm25 = BM25Okapi(tokenized_docs)
    
    # Добавляем BM25 скоры в метаданные документов
    for i, doc in enumerate(original_docs):
        doc.metadata['bm25_scores'] = bm25.get_scores(doc.page_content.split())
    
    return original_docs, bm25


app/config.py

from dotenv import load_dotenv
import os

load_dotenv()

class Settings:
    YANDEXGPT_IAM_TOKEN:str=os.getenv("YANDEXGPT_IAM_TOKEN")
    YANDEXGPT_MODEL_URI:str=os.getenv("YANDEXGPT_MODEL_URI")
    YANDEXGPT_MAIN_MODEL_URI:str=os.getenv("YANDEX_MAIN_MODEL_URI")
    EMBEDDINGS_MODEL:str=os.getenv("EMBEDDINGS_MODEL")
    DEVICE:str=os.getenv("DEVICE")
    PG_CONNECTION:str=os.getenv("PG_CONNECTION")
    PG_COLLECTION_NAME:str=os.getenv("PG_COLLECTION_NAME")
    GIGACHAT_CREDENTIALS:str=os.getenv("GIGACHAT_CREDENTIALS")

settings = Settings()

app/database/__init__.py



app/database/vector_store.py

from functools import lru_cache
from langchain_postgres import PGVector
from ..config import settings
from langchain_community.embeddings import GigaChatEmbeddings
# from langchain.embeddings.base import Embeddings
import time
import requests
from typing import Any, List, Mapping, Optional
# from langchain.callbacks.manager import CallbackManagerForLLMRun
import requests
import langchain
import os
# class YandexGPTEmbeddings(Embeddings):

#     def __init__(self, iam_token=None, api_key=None, folder_id=None, sleep_interval=1):
#         self.iam_token = iam_token
#         self.sleep_interval = sleep_interval
#         self.api_key = api_key
#         self.folder_id = folder_id
#         if self.iam_token:
#             self.headers = {'Authorization': 'Bearer ' + self.iam_token}
#         if self.api_key:
#             self.headers = {'Authorization': 'Api-key ' + self.api_key,
#                              "x-folder-id" : self.folder_id }
                
#     def embed_document(self, text):
#         j = {
#           "model" : "general:embedding",
#           "embedding_type" : "EMBEDDING_TYPE_DOCUMENT",
#           "text": text
#         }
        
#         res = requests.post("https://llm.api.cloud.yandex.net/llm/v1alpha/embedding",
#                             json=j, headers=self.headers)
#         print(res)
#         vec = res.json()['embedding']
#         return vec

#     def embed_documents(self, texts, chunk_size = 0):
#         res = []
#         for x in texts:
#             res.append(self.embed_document(x))
#             time.sleep(self.sleep_interval)
#         return res
        
#     def embed_query(self, text):
#         j = {
#           "model" : "general:embedding",
#           "embedding_type" : "EMBEDDING_TYPE_QUERY",
#           "text": text
#         }
#         res = requests.post("https://llm.api.cloud.yandex.net/llm/v1alpha/embedding",
#                             json=j,headers=self.headers)
#         vec = res.json()['embedding']
#         time.sleep(self.sleep_interval)
#         return vec

@lru_cache
def get_vectorstore() -> PGVector:
    # embeddings = YandexGPTEmbeddings(iam_token=settings.YANDEXGPT_IAM_TOKEN)
    embeddings = GigaChatEmbeddings(credentials=settings.GIGACHAT_CREDENTIALS, verify_ssl_certs=False, scope="GIGACHAT_API_PERS")
    vectorstore = PGVector(
        embeddings=embeddings,
        collection_name=settings.PG_COLLECTION_NAME,
        connection=settings.PG_CONNECTION,
        use_jsonb=True,
        async_mode=True,
    )
    return vectorstore


app/dependencies.py

# app/dependencies.py

from fastapi import Depends
from fastapi import HTTPException
from app.main import app
from app.services.model_loader import ModelLoader

def get_model_loader() -> ModelLoader:
    model_loader = getattr(app.state, "model_loader", None)
    if not model_loader:
        raise HTTPException(status_code=500, detail="Модель ещё не загружена")
    return model_loader


app/graphs/__init__.py



app/graphs/main_graph.py

from typing import Dict, Any, Literal, TypedDict
from langgraph.graph import StateGraph, END
from langgraph.checkpoint.memory import MemorySaver
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables.config import RunnableConfig
from ..schemas.state import State
from ..services.llm import llm_light
from ..logger import setup_logger
from langchain_core.output_parsers import StrOutputParser
from ..database.vector_store import get_vectorstore

logger = setup_logger(__name__)

graph = StateGraph(State)

general_answer = """Данный вопрос определен как не связанный с руководством пользователя, пожалуйста попробуйте снова или обратитесь в наш отдел технической поддержки."""

async def classify_index(state:State) -> State:  # -> Literal["index", "general"]:
    with open("index_summary.txt", "r") as f:
        index = f.read()
    prompt_template = """
Ты - оператор чат-бота, задача которого отвечать на вопросы пользователей, связанные с руководством по приложению. Тебе будет дан текущий запрос пользователя и выжимка из руководства пользователя. Ты должен вернуть ответ строго одним словом: 'Да' или 'Нет'.

Если вопрос пользователя **связан с программой или руководством**, ответь 'Да'.
Если вопрос пользователя **не связан с программой или руководством** (отвлечённая тема), ответь 'Нет'.

**Примеры:**

Выжимка из руководства:

{summary}

Последние сообщения пользователя:

{user_context}

Текущий вопрос пользователя:

1. "Как добавить нового пользователя в систему?"
**Ответ:** Да

2. "Какая погода сегодня?"
**Ответ:** Нет

3. "Как настроить аудит конфигурации ПО?"
**Ответ:** Да

Теперь обработай текущий запрос.

Текущий вопрос пользователя:

{user_message}

Ответ:
"""

    prompt = PromptTemplate.from_template(prompt_template)
    bound = prompt | llm_light | StrOutputParser()
    response:str = await bound.ainvoke({
        "summary":index,
        "user_context":"\n".join([el[1] for el in state["last_messages"] if el[0] == "user"]),
        "user_message":state["user_message"]
    })
    if response.replace("'", "").lower() == "нет" or response.replace("'", "").lower().endswith("нет"):
        return {"is_index":False, "answer":general_answer, "rel_docs":[]}
    else:
        return {"is_index":True}

def route_index(state:State) -> Literal["get_relevant_docs", "__end__"]:
    if state["is_index"]:
        return "get_relevant_docs"
    else:
        return END

# doc_schema:
# {"1.1.1(.1)":{"text":str, "images":list[str]("paths")}}
# {"1.1.1.1":Document()}

async def get_relevant_docs(state:State) -> State:
    vectorstore = get_vectorstore()
    return {'rel_docs':[{el.metadata["chapter"]:{"text":el.metadata["content"], "images":el.metadata["paths"]}} for el in await vectorstore.asimilarity_search(query=state["db_query"])]}

async def score_docs(state:State)->State:
    new_rel_docs=[]
    prompt_template = """
Ты - оператор чат-бота помощника по руководству пользователя. Твоя задача по предоставленному запросу пользователя определить, является ли предложенный раздел документации подходящим.

Отвечай 'Да' если документ релевантен запросу и его можно учитывать при генерации ответа.
Отвечай 'Нет' если документ не поможет пользователю в решении вопроса или он второстепенный.

Важно:

- **Основывайся только на предоставленных данных.**
- **Не добавляй дополнительной информации.**
- **Избегай галлюцинаций.**

**Примеры:**

Текущий вопрос пользователя:

"Как удалить профиль из системы?"

Документ на оценку:

"1.12.9: Удаление профиля - Описывает процесс удаления профиля из системы."

**Ответ:** Да

---

Текущий вопрос пользователя:

"Как изменить настройки браузера?"

Документ на оценку:

"1.6: Авторизация - Описывает процесс входа в систему."

**Ответ:** Нет

Теперь оцени следующий документ.

Текущий вопрос пользователя:

{user_message}

Документ на оценку:

{doc_check}

Ответ:
"""

    prompt = PromptTemplate.from_template(prompt_template)
    bound = prompt | llm_light | StrOutputParser()
    for doc in state["rel_docs"]:
        response = await bound.ainvoke({
            "user_message":state["user_message"],
            "doc_check":list(doc.values())[0]["text"]
        })
        if response.replace("'", "").lower() == "нет" or response.replace("'", "").lower().endswith("нет"):
            continue
        else:
            new_rel_docs.append(doc)
    if len(new_rel_docs) == 0:
        return {"rewrite":True, "rel_docs":[], "retries":state['retries'] + 1}
    else:
        return {"rewrite":False, "rel_docs":new_rel_docs}

contact_message = """Ответ на ваш вопрос не найден среди документов руководства пользователя. Если Вам требуется квалифицированная помощь, позвоните на телефон «горячей линии поддержки», напишите письмо или воспользуйтесь формой регистрации заявки на сайте. 
КОНТАКТНАЯ ИНФОРМАЦИЯ
Техническая поддержка
+7 (495) 258-06-36
info@lense.ru
lense.ru
"""

def route_docs(state:State) -> Literal["rewrite_query", "no_docs", "generate"]:
    if state["retries"] >= 2:
        return "no_docs"
    elif state["rewrite"]:
        return "rewrite_query"
    else:
        return "generate"


async def rewrite_query(state:State) -> State:
    prompt_template = """
Ты - оператор чат-бота помощника по использованию системы управления безопасностью конфигураций ПО. Твоя задача - перефразировать запрос пользователя так, чтобы он привёл к более точным результатам поиска в базе данных.

Важно:

- **Используй термины из руководства пользователя.**
- **Не добавляй информацию, отсутствующую в исходном запросе.**
- **Избегай галлюцинаций.**

**Примеры:**

Краткая выжимка из руководства:

{summary}

Сообщение пользователя:

"Как удалить профиль?"

Неудавшийся запрос:

"Удаление аккаунта"

**Перефразированный запрос:**

"Удаление профиля из системы"

---

Теперь перефразируй текущий запрос.

Сообщение пользователя:

{user_message}

Неудавшийся запрос:

{last_query}

Ответ:
"""

    prompt = PromptTemplate.from_template(prompt_template)
    bound = prompt | llm_light | StrOutputParser()
    with open("index_summary.txt", "r") as f:
        index = f.read()
    return {"db_query": await bound.ainvoke({"summary": index, "user_message":state['user_message'], "last_query":state["db_query"]})}


def no_docs(state:State) -> State:
    return {"answer":contact_message}

async def generate(state:State)-> State:
    prompt_template = """
Ты - чат-бот, цель которого помочь пользователю в использовании платформы. Тебе будут предоставлены история чата, текущий вопрос пользователя и несколько релевантных документов из руководства пользователя. Твоя задача - использовать этот контекст для ответа на вопрос пользователя.

При необходимости, можешь упомянуть изображения, включая их названия или описания, но не придумывай их.

Важно:

- **Основывайся только на предоставленных документах.**
- **Не добавляй информацию, отсутствующую в документах.**
- **Избегай галлюцинаций.**

**Шаблон сообщения для направления пользователя в техподдержку при возникших трудностях:**

{contact_message}

**Релевантные документы:**

{rel_docs}

**Последние сообщения пользователя:**

{user_context}

**Текущий вопрос пользователя:**

{user_message}

Ответ:
"""

    prompt = PromptTemplate.from_template(prompt_template)
    bound = prompt | llm_light | StrOutputParser()
    response:str = await bound.ainvoke({
        "contact_message":contact_message,
        "rel_docs":"\n\n".join([list(el.values())[0]["text"] for el in state['rel_docs']]),
        "user_context":"\n".join([f"{el[0]}: {el[1]}" for el in state["last_messages"]]),
        "user_message":state["user_message"]
    })
    return {"answer":response}

async def score_answer(state:State) -> State:
    prompt_template = """
Ты - оператор чат-бота, задача которого оценить сгенерированный ответ на запрос пользователя. Ты должен вернуть ответ строго одним словом: 'Да' или 'Нет'.

Если ответ чат-бота **удовлетворяет запрос пользователя**, ответь 'Да'.
Если ответ **не решает проблему пользователя или содержит ошибки**, ответь 'Нет'.

**Примеры:**

Релевантные документы:

[Документы, связанные с запросом]

Текущий вопрос пользователя:

"Как добавить новый шаблон конфигурации?"

Ответ чат-бота на оценку:

"Чтобы добавить новый шаблон, перейдите в раздел 'Управление шаблонами' и нажмите 'Создать шаблон'."

**Твоя оценка:** Да

---

Текущий вопрос пользователя:

"Как выключить компьютер?"

Ответ чат-бота на оценку:

"Вы можете выключить компьютер через меню 'Пуск' или нажать кнопку питания."

**Твоя оценка:** Нет

Теперь оцени текущий ответ.

Релевантные документы:

{rel_docs}

Текущий вопрос пользователя:

{user_message}

Ответ чат-бота на оценку:

{answer}

Твоя оценка:
"""

    prompt = PromptTemplate.from_template(prompt_template)
    bound = prompt | llm_light | StrOutputParser()
    response:str = await bound.ainvoke({
        "rel_docs":"\n\n".join([list(el.values())[0]["text"] for el in state['rel_docs']]),
        "user_context":"\n".join([f"{el[0]}: {el[1]}" for el in state["last_messages"]]),
        "user_message":state["user_message"],
        "answer":state["answer"]
    })
    if response.replace("'", "").lower() == "нет" or response.replace("'", "").lower().endswith("нет"):
        return {"rewrite":True, "answer":contact_message, "rel_docs":[],"retries":state["retries"] + 1}
    else:
        return {"rewrite":False}

def route_answer(state:State) -> Literal["rewrite_query", "__end__"]:
    if state["rewrite"]:
        return "rewrite_query"
    return END

graph.add_node(classify_index)
graph.add_node(get_relevant_docs)
graph.add_node(score_docs)
graph.add_node(rewrite_query)
graph.add_node(no_docs)
graph.add_node(generate)
graph.add_node(score_answer)
graph.set_entry_point("classify_index")
graph.add_conditional_edges("classify_index", route_index)
graph.add_edge("get_relevant_docs", "score_docs")
graph.add_conditional_edges("score_docs", route_docs)
graph.add_edge("rewrite_query", "get_relevant_docs")
graph.add_edge("generate", "score_answer")
graph.set_finish_point("no_docs")
graph.add_conditional_edges("score_answer", route_answer)
worker = graph.compile()

app/logger.py

import logging
from logging.handlers import RotatingFileHandler
import os

def setup_logger(name):
    logger = logging.getLogger(name)
    logger.setLevel(logging.INFO)

    # Создаем директорию для логов, если она не существует
    if not os.path.exists('logs'):
        os.makedirs('logs')

    # Настраиваем RotatingFileHandler
    file_handler = RotatingFileHandler(
        'logs/app.log', maxBytes=10485760, backupCount=5)
    file_handler.setLevel(logging.INFO)

    # Настраиваем ConsoleHandler
    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.INFO)

    # Создаем форматтер и добавляем его к обработчикам
    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    file_handler.setFormatter(formatter)
    console_handler.setFormatter(formatter)

    # Добавляем обработчики к логгеру
    logger.addHandler(file_handler)
    logger.addHandler(console_handler)

    return logger


app/main.py

# app/main.py

from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from app.api import endpoints
from app.logger import setup_logger
from app.database.vector_store import get_vectorstore
from app.chunks_creating import documents
from app.services.model_loader import ModelLoader  # Импортируем ModelLoader
from app.chunks_creating import create_annotated_documents, create_bm25_index
from app.state import app_state  # Импортируйте app_state
import asyncio

# Инициализация логгера
logger = setup_logger("app")

# Создание экземпляра FastAPI
app = FastAPI(
    title="CILALLM API",
    description="API для управления чатами и сообщениями с поддержкой изображений",
    version="1.0.0",
)

# Настройка CORS (при необходимости)
origins = [
    "http://localhost",
    "http://localhost:8000",
    # Добавьте другие разрешенные источники
]

app.add_middleware(
    CORSMiddleware,
    allow_origins=origins,            # Разрешенные источники
    allow_credentials=True,
    allow_methods=["*"],              # Разрешенные методы HTTP
    allow_headers=["*"],              # Разрешенные заголовки
)

# Подключение маршрутизатора API с префиксом /api
app.include_router(endpoints.router, prefix="/api")

# Корневой маршрут для проверки работоспособности сервера
@app.get("/")
async def root():
    logger.info("Корневой маршрут был вызван")
    return {"message": "Добро пожаловать в CILALLM API"}

# Пример маршрута для проверки базы данных или других сервисов (по желанию)
@app.get("/health")
async def health_check():
    return {"status": "OK"}

# Инициализация ModelLoader при запуске
@app.on_event("startup")
async def startup_event():
    logger.info("Инициализация модели Qwen2 VL 7b...")
    model_loader = ModelLoader()
    app.state.model_loader = model_loader  # Сохраняем модель в состоянии приложения
    logger.info("Сервер запускается...")

    # Получение аннотированных документов
    annotated_documents = await create_annotated_documents()

    # Разделение для vectorstore и BM25
    vector_documents = [doc for doc in annotated_documents if doc.page_content != doc.metadata['content']]
    bm25_documents = [doc for doc in annotated_documents if doc.page_content == doc.metadata['content']]

    # Индексация в vectorstore
    vectorstore = get_vectorstore()
    await vectorstore.aadd_documents(documents=vector_documents)

    # Создание BM25 индекс
    bm25_documents, bm25 = create_bm25_index(bm25_documents)

    # Сохранение BM25 индекс и документы в app_state
    app_state.bm25 = bm25
    app_state.bm25_documents = bm25_documents

    logger.info("Индексация завершена")
    # Здесь можно добавить инициализацию ресурсов, подключение к другим сервисам и т.д.

@app.on_event("shutdown")
async def shutdown_event():
    logger.info("Сервер останавливается...")
    # Очистка ресурсов, закрытие соединений и т.д.


app/photo_indexes.py

import os

# Полная структура заголовков с ключами в кавычках
titles = {
    "1": "О системе",
    "1.1": "Наименование и обозначение системы",
    "1.2": "Область применения системы",
    "1.3": "Основные функции системы",
    "1.4": "Роли пользователей",
    "2": "Работа в системе",
    "1.5": "Запуск системы",
    "1.6": "Авторизация",
    "1.7": "Просмотр информации о системе",
    "1.8": "Настройка шлюзов автоматизации",
    "1.8.1": "Добавление шлюза",
    "1.8.2": "Статусы шлюза",
    "1.8.3": "Состояния шлюза",
    "1.8.4": "Удаление шлюза",
    "1.9": "Управление учетными записями",
    "1.9.1": "Создание УЗ",
    "1.9.2": "Редактирование УЗ",
    "1.9.3": "Удаление УЗ",
    "1.10": "Иерархия моделей ПО",
    "1.10.1": "Уровни моделей ПО",
    "1.10.2": "Создание модели ПО",
    "1.10.3": "Добавление связей между моделями ПО",
    "1.10.4": "Переименование модели ПО",
    "1.10.5": "Редактирование свойств модели ПО",
    "1.10.6": "Редактирование свойств применимости модели ПО",
    "1.10.7": "Удаление связей между моделями ПО",
    "1.10.8": "Удаление модели ПО",
    "1.11": "Управление шаблонами",
    "1.11.1": "Выбор актуальной версии шаблона",
    "1.11.2": "Загрузка и обновление шаблона",
    "1.11.3": "Создание шаблона",
    "1.11.4": "Выгрузка шаблонов",
    "1.12": "Управление профилями",
    "1.12.1": "Создание профиля из списка профилей",
    "1.12.2": "Копирование профиля",
    "1.12.3": "Создание профиля через перечень шаблонов",
    "1.12.4": "Редактирование требований профиля",
    "1.12.5": "Переименование профиля",
    "1.12.6": "Смена применимости профиля",
    "1.12.7": "Активация профиля",
    "1.12.8": "Архивация профиля",
    "1.12.9": "Удаление профиля",
    "1.13": "Управление требованиями",
    "1.13.1": "Добавление раздела",
    "1.13.2": "Переименование раздела",
    "1.13.3": "Перемещение раздела",
    "1.13.4": "Удаление раздела",
    "1.13.5": "Создание требования",
    "1.13.6": "Добавление сведений",
    "1.13.7": "Добавление применимости",
    "1.13.8": "Добавление данных сбора конфигурации",
    "1.13.9": "Добавление данных анализа конфигурации",
    "1.13.10": "Добавление данных исправления конфигурации",
    "1.13.11": "Добавление требования из Единого реестра требований",
    "1.13.12": "Переименование требования",
    "1.13.13": "Редактирование данных требования",
    "1.13.14": "Смена применимости требования",
    "1.13.15": "Использование отметки «Черновик»",
    "1.13.16": "Удаление требования",
    "1.14": "Управление ресурсами",
    "1.14.1": "Добавление раздела",
    "1.14.2": "Переименование раздела",
    "1.14.3": "Перемещение раздела",
    "1.14.4": "Удаление раздела",
    "1.14.5": "Создание online-ресурса",
    "1.14.6": "Создание offline-ресурса",
    "1.14.7": "Переименование ресурса",
    "1.14.8": "Перемещение ресурса",
    "1.14.9": "Редактирование данных ресурса",
    "1.14.10": "Редактирование программной топологии",
    "1.14.11": "Добавление экземпляра ПО",
    "1.14.11.1": "Добавление экземпляра уровеня 1",
    "1.14.11.2": "Добавление экземпляра уровеня 2",
    "1.14.12": "Редактирование экземпляра ПО",
    "1.14.13": "Импорт и экспорт ресурсов",
    "1.14.13.1": "Содержимое файла экспорта",
    "1.14.13.2": "Импорт ресурса",
    "1.14.13.3": "Экспорт ресурса",
    "1.14.14": "Удаление экземпляра ПО",
    "1.14.15": "Удаление ресурса",
    "1.15": "Аудит конфигурации ПО",
    "1.15.1": "Создание области аудита",
    "1.15.2": "Редактирование области аудита",
    "1.15.3": "Задачи на аудит",
    "1.15.4": "Запуск задачи",
    "1.15.5": "Отмена задачи",
    "1.15.6": "Просмотр отчета аудита",
    "1.15.6.1": "Просмотр отчета",
    "1.15.6.2": "Группировка подзадач",
    "1.15.6.3": "Скачивание отчета",
    "1.15.7": "Просмотр протокола аудита",
    "1.15.7.1": "Просмотр протокола",
    "1.15.7.2": "Статистика проверок",
    "1.15.7.3": "Список требований",
    "1.15.7.4": "Смена отображения требований",
    "1.15.7.5": "Фильтрация требований",
    "1.15.8": "Завершение протокола аудита",
    "1.15.8.1": "Завершение сбора конфигурации",
    "1.15.8.2": "Завершение анализа конфигурации",
    "1.15.9": "Удаление области аудита",
    "3": "Дополнительная информация",
    "1.16": "Сообщения об ошибках",
}

# Путь к папке images
images_folder = "images"

# Получаем список всех файлов в папке images
all_files = os.listdir(images_folder)
print(all_files)


# Функция для проверки, начинается ли имя файла с определенного номера
def get_files_by_number(number):
    return [f for f in all_files if f.startswith(str(number)+'_')]


# Создаем словарь для хранения названий файлов по заголовкам
files_by_titles = {}

# Заполняем словарь
for number in titles:
    files_by_titles[number] = get_files_by_number(number)

# Выводим результат
for title_number, files in files_by_titles.items():
    print(f"Заголовок {title_number}: {titles[title_number]}")
    print(f"Файлы: {files}")


app/preprocessing/__init__.py



app/preprocessing/context_annotator.py

from langchain.prompts import PromptTemplate
from ..services.llm import llm_light

ANNOTATION_PROMPT = """
Ты - помощник, который добавляет важный контекст к фрагментам документации.
Твоя задача - создать краткое описание контекста для данного фрагмента текста.

Текст: {text}

Создай одно предложение, описывающее:
1. К какому разделу руководства относится этот фрагмент
2. Какую основную тему или функциональность он описывает
3. Как он связан с другими частями системы

Формат: "Этот фрагмент относится к [раздел]; описывает [тема]; связан с [связи]."

Контекст:"""

class ContextAnnotator:
    def __init__(self):
        self.prompt = PromptTemplate(
            template=ANNOTATION_PROMPT,
            input_variables=["text"]
        )
        
    async def annotate_chunk(self, chunk_text: str, metadata: dict) -> str:
        """Добавляет контекстную аннотацию к чанку"""
        try:
            # Получаем контекст от модели
            context = await self.prompt.aformat_prompt(text=chunk_text)
            context = await llm_light.ainvoke(context)
            
            # Объединяем контекст с оригинальным текстом
            return f"{context}\n\n{chunk_text}"
        except Exception as e:
            print(f"Ошибка при аннотации чанка: {e}")
            return chunk_text


app/schemas/__init__.py



app/schemas/chat.py

# app/schemas/chat.py
from pydantic import BaseModel
from datetime import datetime
from typing import List

class LastMessageSchema(BaseModel):
    sender: str
    content: str
    timestamp: datetime

class ChatWithLastMessageResponse(BaseModel):
    chat_id: str
    last_message: LastMessageSchema


app/schemas/message.py

# app/schemas/message.py
from pydantic import BaseModel, Field, validator
from typing import List, Dict, Optional
import base64


class MessageImage(BaseModel):
    filename: str
    data: str  # Base64-encoded image data

    @validator('data')
    def validate_base64(cls, v):
        try:
            base64.b64decode(v)
        except Exception:
            raise ValueError('Invalid base64 encoding')
        return v


class SendMessageRequest(BaseModel):
    user_id: str
    chat_id: str
    message: str
    images: Optional[List[MessageImage]] = None  # Опционально список изображений


class PhotoInfo(BaseModel):
    chapter: str
    image_number: str
    base64_data: str  # Base64-encoded image data


class SendMessageResponse(BaseModel):
    user_message: str
    bot_answer: str
    chat_id: str
    photos: List[PhotoInfo]


class GetMessageHistoryResponse(BaseModel):
    sender: str
    content: str
    timestamp: str


app/schemas/state.py

from typing import TypedDict, Tuple


class State(TypedDict):
    user_message: str
    last_messages: list[Tuple]
    is_index: bool
    answer: str
    rel_docs: list[dict]
    db_query: str
    retries: int
    rewrite: bool


app/schemas/user.py

from pydantic import BaseModel


class UserResponse(BaseModel):
    user_id: str


app/services/__init__.py



app/services/llm.py

from ..config import settings
from langchain_community.llms import YandexGPT

llm_light = YandexGPT(iam_token=settings.YANDEXGPT_IAM_TOKEN, model_uri=settings.YANDEXGPT_MODEL_URI)
# llm_pro = YandexGPT(iam_token=settings.YANDEXGPT_IAM_TOKEN, model_uri=settings.YANDEXGPT_MAIN_MODEL_URI)

app/services/model_loader.py

# app/services/model_loader.py

import torch
from transformers import Qwen2VLForConditionalGeneration, AutoProcessor
from qwen_vl_utils import process_vision_info

class ModelLoader:
    def __init__(self):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"Используемое устройство для модели: {self.device}")
        
        # Загрузка модели на устройство
        self.model = Qwen2VLForConditionalGeneration.from_pretrained(
            "Qwen/Qwen2-VL-7B-Instruct",
            torch_dtype=torch.float16,  # Используем float16 для экономии памяти
            device_map="auto"
        )
        self.model.to(self.device)
        
        # Загрузка процессора
        self.processor = AutoProcessor.from_pretrained("Qwen/Qwen2-VL-7B-Instruct")

    def get_model(self):
        return self.model

    def get_processor(self):
        return self.processor


app/state.py

# app/state.py

from typing import Optional, List
from langchain.schema import Document
from rank_bm25 import BM25Okapi

class AppState:
    bm25: Optional[BM25Okapi] = None
    bm25_documents: Optional[List[Document]] = None

app_state = AppState()


core/__init__.py



core/config.py

from pydantic_settings import BaseSettings

class Settings(BaseSettings):
    DATABASE_URL: str
    DB_ECHO: bool = False
    POSTGRES_HOST: str
    POSTGRES_PORT: int
    POSTGRES_USER: str
    POSTGRES_PASSWORD: str
    POSTGRES_DB: str
    # Добавьте другие необходимые настройки

    class Config:
        env_file = "db.env"

settings = Settings()


core/models/__init__.py

# core/models/__init__.py
from .user import User
from .chat import Chat
from .message import Message

__all__ = ["User", "Chat", "Message"]


core/models/base.py

from sqlalchemy.orm import DeclarativeBase, Mapped, mapped_column, declared_attr

class Base(DeclarativeBase):
    __abstract__ = True

    @declared_attr.directive
    def __tablename__(cls) -> str:
        return f"{cls.__name__.lower()}s"

    id: Mapped[int] = mapped_column(primary_key=True, autoincrement=True)


core/models/chat.py

# core/models/chat.py
from sqlalchemy.orm import Mapped, mapped_column, relationship
from sqlalchemy import ForeignKey  # Импортируйте ForeignKey
from .base import Base
from typing import TYPE_CHECKING

if TYPE_CHECKING:
    from .user import User
    from .message import Message

class Chat(Base):
    __tablename__ = "chats"
    
    id: Mapped[int] = mapped_column(primary_key=True, autoincrement=True)
    user_id: Mapped[int] = mapped_column(ForeignKey("users.id"), nullable=False)  # Добавлен ForeignKey
    user: Mapped["User"] = relationship("User", back_populates="chats")
    messages: Mapped[list["Message"]] = relationship("Message", back_populates="chat")


core/models/crud.py

from typing import Optional, List, Sequence

from sqlalchemy import func, select, desc
from sqlalchemy.ext.asyncio import AsyncSession

from app.schemas.chat import ChatWithLastMessageResponse, LastMessageSchema
from core.models import User, Chat, Message


async def create_user(session: AsyncSession, user_id: str) -> User:
    user = User(user_id=user_id)
    session.add(user)
    await session.commit()
    await session.refresh(user)
    return user


async def get_user_by_user_id(session: AsyncSession, user_id: int) -> Optional[User]:
    result = await session.execute(select(User).where(User.id == user_id))
    return result.scalar_one_or_none()


async def create_chat(session: AsyncSession, user: User) -> Chat:
    chat = Chat(user_id=user.id)
    session.add(chat)
    await session.commit()
    await session.refresh(chat)
    return chat


async def create_message(session: AsyncSession, chat: Chat, sender: str, content: str) -> Message:
    message = Message(chat_id=chat.id, sender=sender, content=content)
    session.add(message)
    await session.commit()
    await session.refresh(message)
    return message


async def get_last_five_chats_with_last_message(session: AsyncSession, user_id: int, limit: int = 5) -> (List)[ChatWithLastMessageResponse]:
    """
    Получает последние пять чатов пользователя вместе с последним сообщением в каждом чате.
    """
    # Подзапрос для получения времени последнего сообщения в каждом чате
    subquery = (
        select(
            Message.chat_id,
            func.max(Message.timestamp).label("last_message_time")
        )
        .join(Chat, Chat.id == Message.chat_id)
        .join(User, User.id == Chat.user_id)
        .where(User.id == user_id)
        .group_by(Message.chat_id)
        .subquery()
    )

    # Основной запрос для получения чатов и их последних сообщений
    stmt = (
        select(Chat, Message)
        .join(subquery, Chat.id == subquery.c.chat_id)
        .join(Message, (Message.chat_id == Chat.id) & (Message.timestamp == subquery.c.last_message_time))
        .order_by(desc(subquery.c.last_message_time))
        .limit(limit)
    )

    result = await session.execute(stmt)
    rows = result.fetchall()

    chats_with_last_messages = []
    for chat, message in rows:
        last_message = LastMessageSchema(
            sender=message.sender,
            content=message.content,
            timestamp=message.timestamp
        )
        chat_response = ChatWithLastMessageResponse(
            chat_id=chat.id,
            last_message=last_message
        )
        chats_with_last_messages.append(chat_response)

    return chats_with_last_messages


async def get_last_n_messages(session: AsyncSession, chat: Chat, limit: int = 5) -> List[Message]:
    """
    Получает последние N сообщений из чата.
    """
    stmt = select(Message).where(Message.chat_id == chat.id).order_by(desc(Message.timestamp)).limit(limit)
    result = await session.execute(stmt)
    messages = result.scalars().all()
    return list(reversed(messages))  # Возвращаем в хронологическом порядке


async def get_all_chat_history_by_chat_id(db: AsyncSession, chat_id: int) -> Sequence[Message]:
    query = (select(Message).where(Message.chat_id == chat_id)
             .order_by(Message.timestamp.desc()))
    result = await db.execute(query)
    return result.scalars().all()

core/models/db_helper.py

from sqlalchemy.ext.asyncio import (
    AsyncSession,
    create_async_engine,
    async_sessionmaker,
    async_scoped_session,
)
from core.config import settings
from asyncio import current_task

class DatabaseHelper:
    def __init__(self, url: str, echo: bool = False):
        self.engine = create_async_engine(url=url, echo=echo)
        self.session_factory = async_sessionmaker(
            bind=self.engine,
            autoflush=False,
            autocommit=False,
            expire_on_commit=False,
        )

    async def get_session(self) -> AsyncSession:
        async with self.session_factory() as session:
            yield session
    
    def get_scoped_session(self):
        session = async_scoped_session(
            session_factory=self.session_factory,
            scopefunc=current_task,
        )
        return session

    async def session_dependency(self) -> AsyncSession:
        async with self.session_factory() as session:
            yield session
            await session.close()

    async def scoped_session_dependency(self) -> AsyncSession:
        session = self.get_scoped_session()
        yield session
        await session.close()

db_helper = DatabaseHelper(url=settings.DATABASE_URL, echo=settings.DB_ECHO)


core/models/message.py

# core/models/message.py
from sqlalchemy.orm import Mapped, mapped_column, relationship
from sqlalchemy import ForeignKey  # Импортируйте ForeignKey
from .base import Base
from datetime import datetime
from typing import TYPE_CHECKING

if TYPE_CHECKING:
    from .chat import Chat

class Message(Base):
    __tablename__ = "messages"
    
    id: Mapped[int] = mapped_column(primary_key=True, autoincrement=True)
    chat_id: Mapped[int] = mapped_column(ForeignKey("chats.id"), nullable=False)  # Добавлен ForeignKey
    sender: Mapped[str] = mapped_column()  # 'user' или 'bot'
    content: Mapped[str] = mapped_column()
    timestamp: Mapped[datetime] = mapped_column(default=datetime.utcnow)
    chat: Mapped["Chat"] = relationship("Chat", back_populates="messages")


core/models/user.py

# core/models/user.py
from sqlalchemy.orm import Mapped, mapped_column, relationship
from .base import Base
from typing import TYPE_CHECKING
if TYPE_CHECKING:
    from .chat import Chat
class User(Base):
    __tablename__ = "users"
    
    id: Mapped[int] = mapped_column(primary_key=True, autoincrement=True)
    user_id: Mapped[str] = mapped_column(unique=True, index=True)
    chats: Mapped[list["Chat"]] = relationship("Chat", back_populates="user")


docker-compose.yml

version: '3.8'

services:
  db:
    image: ankane/pgvector:latest  # Образ PostgreSQL с предустановленным pgvector
    container_name: postgres
    environment:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
      POSTGRES_DB: cilallm  # Замените на имя вашей базы данных
    ports:
      - "5432:5432"
    volumes:
      - db_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 10s
      timeout: 5s
      retries: 5

  app:
    build: .
    container_name: fastapi_app
    depends_on:
      db:
        condition: service_healthy
    env_file:
      - .env
    ports:
      - "8000:8000"
    volumes:
      - .:/app
    entrypoint: ["/app/entrypoint.sh"]
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]
    # Для Docker Compose версии < 3.8 используйте `runtime: nvidia`
    runtime: nvidia  # Добавляем поддержку NVIDIA runtime
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility

volumes:
  db_data:


entrypoint.sh

#!/usr/bin/env bash
# entrypoint.sh

# Ожидаем готовности базы данных
./wait-for-it.sh db 5432 --timeout=60 --strict -- echo "База данных доступна"

# Выполняем миграции Alembic
echo "Выполнение миграций базы данных..."
alembic upgrade head

# Запускаем сервер Uvicorn
echo "Запуск сервера..."
uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload


requirements.txt

python-dotenv
fastapi
uvicorn[standard]
sqlalchemy[asyncio]
asyncpg
aiofiles
pydantic-settings
alembic
psycopg2-binary
gigachat
langchain
langgraph
langchain-community
langchain-postgres
langchain-core
yandexcloud
python-docx
httpx
pillow
torch>=2.0.0
transformers
qwen_vl_utils
psycopg2-binary
gigachat
rank-bm25
tqdm


wait-for-it.sh

#!/usr/bin/env bash
# wait-for-it.sh

set -e

host="$1"
port="$2"
shift 2
cmd="$@"

until nc -z "$host" "$port"; do
  >&2 echo "Сервис $host:$port недоступен - ожидаем..."
  sleep 5
done

>&2 echo "Сервис $host:$port доступен - продолжаем запуск $cmd"
exec $cmd
